{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItsFreakinDay/hybridCompSys/blob/task2/Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6wByOrUtj6Y",
        "outputId": "23603ab2-62a2-4689-9a76-f401216ffe16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing secondTask.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile secondTask.cu\n",
        "#include <torch/extension.h>\n",
        "\n",
        "#include <torch/extension.h>\n",
        "\n",
        "__global__ void d_divide(float* a, int scalar, int* c, int n) {\n",
        "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "\n",
        "    if (i < n) {\n",
        "        c[i] = static_cast<int>(a[i]) / scalar;\n",
        "    }\n",
        "}\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
        "\n",
        "const int block_size = 128;\n",
        "\n",
        "__forceinline__ int calc_grid_size(int m) {\n",
        "    return (m + block_size - 1) / block_size;\n",
        "}\n",
        "\n",
        "torch::Tensor divide_by_scalar(torch::Tensor a, int scalar) {\n",
        "    CHECK_INPUT(a);\n",
        "\n",
        "    auto host_result = torch::zeros_like(a, torch::kInt);\n",
        "    int n = a.numel();\n",
        "\n",
        "    d_divide<<<calc_grid_size(n), block_size>>>(\n",
        "        a.data_ptr<float>(),\n",
        "        scalar,\n",
        "        host_result.data_ptr<int>(),\n",
        "        n\n",
        "    );\n",
        "\n",
        "    return host_result;\n",
        "}\n",
        "\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"my_divide_by_scalar\", &divide_by_scalar, \"Custom divide by scalar operation\");\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoGzgD8K6eyr",
        "outputId": "061c7deb-033c-433c-c088-87e09096328b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/307.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SecondTask.py\n",
        "import unittest\n",
        "import torch\n",
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "class LabTest(unittest.TestCase):\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        cls.ext = load(\n",
        "            name='my_extension',\n",
        "            sources=['secondTask.cu'],\n",
        "            extra_cuda_cflags=['-O3'],\n",
        "            extra_cflags=['-O3'],\n",
        "        )\n",
        "        cls.rtol = 1e-5\n",
        "        cls.atol = 1e-5\n",
        "\n",
        "    def test_divide_by_scalar(self):\n",
        "        n = torch.randint(low=1, high=2048, size=(1, 1))\n",
        "        scalar = torch.randint(low=1, high=100, size=(1, 1)).item()\n",
        "\n",
        "        x = torch.randint(low=1, high=100, size=(n.item(),)).to(\"cuda\").float()\n",
        "        z = LabTest.ext.my_divide_by_scalar(x, scalar)\n",
        "        z_ = x // scalar\n",
        "\n",
        "        torch.testing.assert_allclose(z, z_, rtol=self.rtol, atol=self.atol)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0MvK6f_jy9m",
        "outputId": "578397b4-7a54-46cb-ca59-f87984bc375d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting SecondTask.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run SecondTask.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wMF_ETZnJme",
        "outputId": "db9e026b-17d4-4acf-9241-c1fe0100cc08"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/SecondTask.py:25: FutureWarning: `torch.testing.assert_allclose()` is deprecated since 1.12 and will be removed in a future release. Please use `torch.testing.assert_close()` instead. You can find detailed upgrade instructions in https://github.com/pytorch/pytorch/issues/61844.\n",
            "  torch.testing.assert_allclose(z, z_, rtol=self.rtol, atol=self.atol)\n",
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.041s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnCxRiQqRazMzdFvuRSiEr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}